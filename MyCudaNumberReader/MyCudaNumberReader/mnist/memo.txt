	////matrix test
	//float a[12];// 4 x 3
	//for (int i = 0; i < 12; i++) {
	//	a[i] = i + 1;
	//}

	//float b[15];// 3 x 5
	//for (int i = 0; i < 15; i++) {
	//	b[i] = i + 1;
	//}

	//float c[20];

	//float* adev;
	//float* bdev;
	//float* cdev;

	//cudaMalloc((void**)&adev, 12 * sizeof(float));
	//cudaMemcpy(adev, a, 12 * sizeof(float), cudaMemcpyHostToDevice);
	//cudaMalloc((void**)&bdev, 15 * sizeof(float));
	//cudaMemcpy(bdev, b, 15 * sizeof(float), cudaMemcpyHostToDevice);
	//cudaMalloc((void**)&cdev, 20 * sizeof(float));

	//matrix_multiplication << <4, 5 >> > (adev, 4, 3, bdev, 3, 5, cdev);
	//cudaMemcpy(c, cdev, 20 * sizeof(float), cudaMemcpyDeviceToHost);
	//cout << cudaGetErrorString(cudaGetLastError()) << endl;
	//for (int i = 0; i < 20; i++) {
	//	cout << c[i] << ' ';
	//}
	//cout << endl;
	//return 0;
	
	//transpose test
	float a[12];// 4 x 3
	float at[12];
	for (int i = 0; i < 12; i++) {
		a[i] = i + 1;
	}	

	float* adev;
	float* atdev;

	cudaMalloc((void**)&adev, 12 * sizeof(float));
	cudaMemcpy(adev, a, 12 * sizeof(float), cudaMemcpyHostToDevice);
	cudaMalloc((void**)&atdev, 12 * sizeof(float));

	matrix_transpose << <4, 3 >> > (adev, 4, 3, atdev);	
	cout << cudaGetErrorString(cudaGetLastError()) << endl;
	cudaMemcpy(at, atdev, 12 * sizeof(float), cudaMemcpyDeviceToHost);
	for (int i = 0; i < 12; i++) {
		cout << at[i] << ' ';		
	}
	cout << endl;
	return 0;
	
	
	
	cudaMemcpy(levels[0], image, INPUT_SIZE * sizeof(float), cudaMemcpyHostToDevice);

	matrix_multiplication << <1, ws_w_sz[0] >> > (levels[0], 1, INPUT_SIZE, wss_dev[0], ws_h_sz[0], ws_w_sz[0], levels[1]);
	bias_sigmoid << <1, ws_w_sz[0] >> > (levels[1], bss_dev[0]);

	matrix_multiplication << <1, ws_w_sz[1] >> > (levels[1], 1, ws_w_sz[0], wss_dev[1], ws_h_sz[1], ws_w_sz[1], levels[2]);
	bias_sigmoid << <1, ws_w_sz[1] >> > (levels[2], bss_dev[1]);

	matrix_multiplication << <1, ws_w_sz[2] >> > (levels[2], 1, ws_w_sz[1], wss_dev[2], ws_h_sz[2], ws_w_sz[2], levels[3]);
	bias_sigmoid << <1, ws_w_sz[2] >> > (levels[3], bss_dev[2]);

	softmax << <1, 1 >> > (levels[2], output_dev, ws_w_sz[2]);
	cudaMemcpy(output, output_dev, ws_w_sz[2] * sizeof(float), cudaMemcpyDeviceToHost);
	
	
	
	predict(img, label);

	output[label]--;
	cudaMemcpy(swl, output, 10 * sizeof(float), cudaMemcpyHostToDevice);
	output[label]++;

	matrix_multiplication << <1, ws_h_sz[2] >> > (swl, 1, ws_w_sz[2], wsst[2], ws_w_sz[2], ws_h_sz[2], xssd[2]); //xssd[2] = 1 x 100
	matrix_multiplication << <ws_w_sz[1], ws_w_sz[2] >> > (levels[2], ws_w_sz[1], 1, swl, 1, ws_w_sz[2], wssd[2]); // wssd[2] = 100 x 10
	sigmoid_backward << <1, ws_w_sz[2] >> > (xssd[2], levels[2], 1, ws_w_sz[1]);

	matrix_multiplication << <1, ws_h_sz[2] >> > (xssd[2], 1, ws_w_sz[1], wsst[1], ws_w_sz[1], ws_h_sz[1], xssd[1]); //xssd[1] = 1 x 50
	matrix_multiplication << <1, ws_h_sz[2] >> > (levels[1], ws_w_sz[0], 1, xssd[2], 1, ws_w_sz[1], wssd[1]); //wssd[1] = 50 x 100
	sigmoid_backward << <1, ws_w_sz[1] >> > (xssd[1], levels[1], 1, ws_w_sz[0]);

	matrix_multiplication << <1, ws_h_sz[2] >> > (levels[0], ws_h_sz[0], 1, xssd[1], 1, ws_w_sz[0], wssd[0]); // wssd[0] = 784 x 50

	for (j = 0; j < LEVEL_SIZE; j++) {
		set_weight_changes << < ws_h_sz[j], ws_w_sz[j] >> > (wss_dev[j], wssd[j], ws_h_sz[j], ws_w_sz[j]);
	}

	set_bias_changes << <1, ws_w_sz[0] >> > (bss_dev[0], xssd[1], ws_w_sz[0]);
	set_bias_changes << <1, ws_w_sz[1] >> > (bss_dev[1], xssd[2], ws_w_sz[1]);
	set_bias_changes << <1, ws_w_sz[2] >> > (bss_dev[2], swl, ws_w_sz[2]);
	
	cout << cudaGetErrorString(cudaGetLastError()) << endl;
	
		cout << cudaGetErrorString(cudaGetLastError()) << endl;
		print_matrix << <1, 1 >> > (wsst[1], 5000);
	
	
layer 0 = 1 x 784
w = 784 x 50

layer 1 = 1 x 50
w = 50 x 100

layer 2 = 1 x 100
w = 100 x 10

layer 3 = 1 x 10

bss = new float* [LEVEL_SIZE];
bs_arr_sz = new int[LEVEL_SIZE] { 100, 200, 10 };

wss = new float* [LEVEL_SIZE];
ws_h_sz = new int[LEVEL_SIZE] { 784, 50, 100 };
ws_w_sz = new int[LEVEL_SIZE] { 50, 100, 10 };

h = 784 50 100
w = 50 100 10

0   1  2   3
784 50 100 10










void predict(float* image, int label) {
	int i;

	cudaMemcpy(levels[0], image, level_size[0] * sizeof(float), cudaMemcpyHostToDevice);

	matrix_multiplication << <1, level_size[1] >> > (levels[0], 1, level_size[0], wss_dev[0], level_size[0], level_size[1], levels[1]);
	bias_sigmoid << <1, level_size[1] >> > (levels[1], bss_dev[0]);

	matrix_multiplication << <1, level_size[2] >> > (levels[1], 1, level_size[1], wss_dev[1], level_size[1], level_size[2], levels[2]);
	bias_sigmoid << <1, level_size[2] >> > (levels[2], bss_dev[1]);

	matrix_multiplication << <1, level_size[3] >> > (levels[2], 1, level_size[2], wss_dev[2], level_size[2], level_size[3], levels[3]);
	softmax << <1, 1 >> > (levels[3], output_dev, level_size[3]);
	cudaMemcpy(output, output_dev, level_size[3] * sizeof(float), cudaMemcpyDeviceToHost);

	memset(ans, 0, level_size[3] * sizeof(float));
	ans[label] = 1;

	cee = cross_entropy_error(output, ans, level_size[3]);

	int maxidx = -1;
	float max = -1;
	for (i = 0; i < level_size[3]; i++) {
		if (output[i] > max) {
			max = output[i];
			maxidx = i;
		}
	}

	anstmp = maxidx;
}

void backprop(float* img, int label) {
	int i, j;

	for (i = 0; i < LEARN_COUNT; i++) {
		predict(img, label);
		//cout << cee << endl;
		for (j = 0; j < LEVEL_SIZE; j++) {
			matrix_transpose << <level_size[j], level_size[j + 1] >> > (wss_dev[j], level_size[j], level_size[j + 1], wsst[j]);
		}

		output[label]--;
		cudaMemcpy(xssd[3], output, level_size[3] * sizeof(float), cudaMemcpyHostToDevice);
		output[label]++;

		matrix_multiplication << <1, level_size[2] >> > (xssd[3], 1, level_size[3], wsst[2], 10, level_size[2], xssd[2]); //xssd[2] = 1 x 100		
		matrix_multiplication << <level_size[2], level_size[3] >> > (levels[2], level_size[2], 1, xssd[3], 1, level_size[3], wssd[2]); //wssd[2] = 100 x 10
		sigmoid_backward << <1, level_size[2] >> > (xssd[2], levels[2], 1, level_size[2]);

		matrix_multiplication << <1, level_size[1] >> > (xssd[2], 1, level_size[2], wsst[1], level_size[2], level_size[1], xssd[1]); //xssd[1] = 1 x 50
		matrix_multiplication << <level_size[1], level_size[2] >> > (levels[1], level_size[1], 1, xssd[2], 1, level_size[2], wssd[1]); //wssd[1] = 50 x 100
		sigmoid_backward << <1, level_size[1] >> > (xssd[1], levels[1], 1, level_size[1]);

		matrix_multiplication << <level_size[0], level_size[1] >> > (levels[0], level_size[0], 1, xssd[1], 1, level_size[1], wssd[0]); //wssd[0] = 784 x 50

		for (j = 0; j < LEVEL_SIZE; j++) {
			set_weight_changes << < level_size[j], level_size[j + 1] >> > (wss_dev[j], wssd[j], level_size[j], level_size[j + 1]);
			set_bias_changes << <1, level_size[j + 1] >> > (bss_dev[j], xssd[j + 1], level_size[j + 1]);
		}
	}
}

	float t1[16];
	for (int i = 0; i < 16; i++) {
		t1[i] = i + 1;
	}
	float t2[4];
	for (int i = 0; i < 4; i++) {
		t2[i] = i + 1;
	}
	float* dev1;
	float* dev2;
	float* devr;
	cudaMalloc((void**)&dev1, 16 * sizeof(float));
	cudaMalloc((void**)&dev2, 4 * sizeof(float));
	cudaMalloc((void**)&devr, 9 * sizeof(float));
	cudaMemcpy(dev1, t1, 16 * sizeof(float), cudaMemcpyHostToDevice);
	cudaMemcpy(dev2, t2, 4 * sizeof(float), cudaMemcpyHostToDevice);
	cnnFilter << <3, 3 >> > (dev1, 4, dev2, 2, devr);
	print_matrix << <1, 1 >> > (dev1, 16, 4);
	print_matrix << <1, 1 >> > (dev2, 4, 2);
	print_matrix << <1, 1 >> > (devr, 9, 3);
	return 0;
	
	
		//free memory
	for (i = 0; i < LEVEL_SIZE; i++) {
		cudaFree(bss_dev[i]);
		cudaFree(wss_dev[i]);
		cudaFree(wsst[i]);
		cudaFree(wssd[i]);
		cudaFree(level[i]);
		cudaFree(xssd[i]);
	}
	cudaFree(level[3]);
	cudaFree(xssd[3]);

	cudaFree(bss_dev);
	cudaFree(wss_dev);
	cudaFree(wsst);
	cudaFree(wssd);
	cudaFree(level);
	cudaFree(xssd);

	for (i = 0; i < FILTER_COUNT; i++) {
		cudaFree(cnn[i]);
		cudaFree(cnn_w[i]);
		cudaFree(cnn_b[i]);
		cudaFree(cnn_b_w[i]);
		cudaFree(filter[i]);
	}
	cudaFree(cnn);
	cudaFree(cnn_w);
	cudaFree(cnn_b);
	cudaFree(cnn_b_w);
	cudaFree(filter);
	//~free memory
	
	
	
	//#include "cuda_runtime.h"
//#include "device_launch_parameters.h"
//
//#include <iostream>
//#include <stdio.h>
//
//using namespace std;
//
//cudaError_t addWithCuda(int* c, const int* a, const int* b, unsigned int size);
//
//__global__ void addKernel(int* c, const int* a, const int* b)
//{
//    int i = threadIdx.x;
//    c[i] = a[i] + b[i];
//}
//
//int main()
//{
//    const int arraySize = 5;
//    const int a[arraySize] = { 1, 2, 3, 4, 5 };
//    const int b[arraySize] = { 10, 20, 30, 40, 50 };
//    int c[arraySize] = { 0 };
//
//    // Add vectors in parallel.
//    cudaError_t cudaStatus = addWithCuda(c, a, b, arraySize);
//    if (cudaStatus != cudaSuccess) {
//        fprintf(stderr, "addWithCuda failed!");
//        return 1;
//    }
//
//    printf("{1,2,3,4,5} + {10,20,30,40,50} = {%d,%d,%d,%d,%d}\n",
//        c[0], c[1], c[2], c[3], c[4]);
//
//    // cudaDeviceReset must be called before exiting in order for profiling and
//    // tracing tools such as Nsight and Visual Profiler to show complete traces.
//    cudaStatus = cudaDeviceReset();
//    if (cudaStatus != cudaSuccess) {
//        fprintf(stderr, "cudaDeviceReset failed!");
//        return 1;
//    }
//
//    return 0;
//}
//
//// Helper function for using CUDA to add vectors in parallel.
//cudaError_t addWithCuda(int* c, const int* a, const int* b, unsigned int size)
//{
//    int* dev_a = 0;
//    int* dev_b = 0;
//    int* dev_c = 0;
//    cudaError_t cudaStatus;
//
//    // Choose which GPU to run on, change this on a multi-GPU system.
//    cudaStatus = cudaSetDevice(0);
//    if (cudaStatus != cudaSuccess) {
//        fprintf(stderr, "cudaSetDevice failed!  Do you have a CUDA-capable GPU installed?");
//        goto Error;
//    }
//
//    // Allocate GPU buffers for three vectors (two input, one output)    .
//    cudaStatus = cudaMalloc((void**)&dev_c, size * sizeof(int));
//    if (cudaStatus != cudaSuccess) {
//        fprintf(stderr, "cudaMalloc failed!");
//        goto Error;
//    }
//
//    cudaStatus = cudaMalloc((void**)&dev_a, size * sizeof(int));
//    if (cudaStatus != cudaSuccess) {
//        fprintf(stderr, "cudaMalloc failed!");
//        goto Error;
//    }
//
//    cudaStatus = cudaMalloc((void**)&dev_b, size * sizeof(int));
//    if (cudaStatus != cudaSuccess) {
//        fprintf(stderr, "cudaMalloc failed!");
//        goto Error;
//    }
//
//    // Copy input vectors from host memory to GPU buffers.
//    cudaStatus = cudaMemcpy(dev_a, a, size * sizeof(int), cudaMemcpyHostToDevice);
//    if (cudaStatus != cudaSuccess) {
//        fprintf(stderr, "cudaMemcpy failed!");
//        goto Error;
//    }
//
//    cudaStatus = cudaMemcpy(dev_b, b, size * sizeof(int), cudaMemcpyHostToDevice);
//    if (cudaStatus != cudaSuccess) {
//        fprintf(stderr, "cudaMemcpy failed!");
//        goto Error;
//    }
//
//    // Launch a kernel on the GPU with one thread for each element.
//    addKernel << <1, size >> > (dev_c, dev_a, dev_b);
//
//    // Check for any errors launching the kernel
//    cudaStatus = cudaGetLastError();
//    if (cudaStatus != cudaSuccess) {
//        fprintf(stderr, "addKernel launch failed: %s\n", cudaGetErrorString(cudaStatus));
//        goto Error;
//    }
//
//    // cudaDeviceSynchronize waits for the kernel to finish, and returns
//    // any errors encountered during the launch.
//    cudaStatus = cudaDeviceSynchronize();
//    if (cudaStatus != cudaSuccess) {
//        fprintf(stderr, "cudaDeviceSynchronize returned error code %d after launching addKernel!\n", cudaStatus);
//        goto Error;
//    }
//
//    // Copy output vector from GPU buffer to host memory.
//    cudaStatus = cudaMemcpy(c, dev_c, size * sizeof(int), cudaMemcpyDeviceToHost);
//    if (cudaStatus != cudaSuccess) {
//        fprintf(stderr, "cudaMemcpy failed!");
//        goto Error;
//    }
//
//Error:
//    cudaFree(dev_c);
//    cudaFree(dev_a);
//    cudaFree(dev_b);
//
//    return cudaStatus;
//}

//CNN 편향은 앞에서 흘러들어온 dx matrix의 합

        error = cudaMalloc((void**)&weight, weight_size * sizeof(float));
        if (error != cudaSuccess) {
            throw exception(cudaGetErrorString(error));
        }

        error = cudaMemset(weight, 0, weight_size * sizeof(float));
        if (error != cudaSuccess) {
            throw exception(cudaGetErrorString(error));
        }

class
input - cnn_w[0] - <cnn[0](data_grad) - cnn_w[1](weight_grad)> - cnn[1](delta) - pooling

	//test-code
	//aff forward
	//layer* input = new layer(AFF, 1, 784, 784, 50);	
	//input->set_data(images->at(0), 784);
	//input->generate_weight(0.5, sqrt(2 / 50.0));

	//layer* level1 = new layer(AFF, 1, 50, 50, 100);	
	//level1->generate_weight(0.5, sqrt(2 / 100.0));

	//input->forward(level1->data);

	//input->print_data(28);
	//input->print_weight();
	//level1->print_data();
	//~aff forward

	//cnn forward
	//layer* input = new layer(CNN, 28, 28, 5, 5);
	//input->set_data(images->at(0), 784);
	//input->generate_weight(0.5, sqrt(2 / 50.0));

	//layer* level1 = new layer(CNN, 24, 24, 5, 5);
	//level1->generate_weight(0.5, sqrt(2 / 100.0));

	//input->forward(level1->data);

	//input->print_data(28);
	//input->print_weight(5);
	//level1->print_data(24);
	//~cnn forward
	//~test-code
	
		//input->print_data();
		//input->print_weight();
		//conv1->print_data();
		//conv1->print_weight();
		//aff1->print_data();
		//aff1->print_weight();
		//output->print_data();		
		//output->print_weight();

		cout << "output : ";
		for (j = 0; j < 10; j++) {
			cout << output->output_2[j] << ' ';
		}
		cout << endl;

		cout << "answer : ";
		for (j = 0; j < 10; j++) {
			cout << output->answer[j] << ' ';
		}
		cout << endl;
		
		//d : delta, a : pooling data, m : pooling size, b : cnn data, n : cnn size, c : pooling backward result
		
		
		
		
		
		
		
		
		
	layer* input = new layer(CONV, 28, 28, 5, 5);
	layer* conv1 = new layer(CONV, 24, 24, 5, 5);	
	layer* pool1 = new layer(POOLING, 20, 20, 2, 2);
	layer* result = new layer(RESULT, 1, 100, 100, 10);

	input->lr = LEARN_RATE;
	conv1->lr = LEARN_RATE;
	pool1->lr = LEARN_RATE;
	result->lr = LEARN_RATE;

	//layer* input = new layer(CONV, 28, 28, 3, 3);
	//layer* conv1 = new layer(CONV, 26, 26, 3, 3);
	//layer* pool1 = new layer(POOLING, 24, 24, 2, 2);

	//layer* conv2 = new layer(CONV, 12, 12, 3, 3);
	//layer* conv3 = new layer(CONV, 10, 10, 3, 3);
	//layer* pool2 = new layer(POOLING, 8, 8, 2, 2);

	//layer* result = new layer(RESULT, 1, 16, 16, 10);

	output* out = new output(10);

	for (i = 0; i < images->size(); i++) {
		input->set_data(images->at(i), 784); //set input		
		out->set_answer(answers->at(i));

		for (j = 0; j < LEARN_COUNT; j++) {

			//predict
			input->forward(conv1);
			conv1->forward(pool1);
			pool1->forward(result);
			result->forward(out);

			////predict
			//input->forward(conv1);
			//conv1->forward(pool1);
			//pool1->forward(conv2);
			//conv2->forward(conv3);
			//conv3->forward(pool2);
			//pool2->forward(result);
			//result->forward(out);

			out->forward();
			if (j == 0) {
				cout << "case " << i << " ans " << labels->at(i) << " cee-> " << out->cross_entrophy_error;
			}
			else if (j == LEARN_COUNT - 1 || out->cross_entrophy_error < 0.1) {
				cout << " to " << out->cross_entrophy_error << endl;
				break;
			}
			out->backward();
			
			//backprop
			result->backward(out);
			pool1->backward(result);
			conv1->backward(pool1);
			input->backward(conv1);

			////backprop
			//result->backward(out);
			//pool2->backward(result);
			//conv3->backward(pool2);
			//conv2->backward(conv3);
			//pool1->backward(conv2);
			//conv1->backward(pool1);
			//input->backward(conv1);
		}
		//print_data(input->data, 28);		
	}
		
		for (i = 0; i < images->size(); i++) {
		input->set_data(images->at(i), 784);
		out->set_answer(answers->at(i));

		//predict
		input->forward(conv1);		
		conv1->forward(pool1);		
		pool1->forward(result);		
		result->forward(out);

		////predict
		//input->forward(conv1);
		//conv1->forward(pool1);
		//pool1->forward(conv2);
		//conv2->forward(conv3);
		//conv3->forward(pool2);
		//pool2->forward(result);
		//result->forward(out);

		out->forward();
		if (j == LEARN_COUNT - 1) {
			//out->print_info();
		}

		max_idx = -1;
		max_val = -9;
		for (k = 0; k < out->data_size; k++) {
			tmp = out->data_after_softmax[k];
			if (tmp > max_val) {
				max_val = tmp;
				max_idx = k;
			}
		}
		cout << "task " << i << "/ans : " << labels->at(i) << "/res : " << max_idx << endl;
		if (labels->at(i) == max_idx) {
			correct++;
		}
	}
	
	
	
	
	
	
	
	
	void layer::batch_norm_backward(float* delta, float* _data, int size) {
	cudaError_t error;
	int i, j, k;

	float* delta_host = new float[size];
	cpy_host(delta_host, delta, size);	

	float* data_host = new float[size];
	cpy_host(data_host, _data, size);

	db = 0;
	for (i = 0; i < size; i++) {
		db += delta_host[i];
	}

	dg = 0;
	for (i = 0; i < size; i++) {
		dg += (data_host[i] - b) / g * delta_host[i];
	}

	float* dxu1 = new float[size];
	float* divar = new float[size]; //ivar = dist^2 - 10e-7
	float ivar = 1 / sqrt(dst_dist * dst_dist - 10e-7);
	for (i = 0; i < size; i++) {
		dxu1[i] = delta_host[i] * ivar;
		divar[i] = delta_host[i] * (data_host[i] - dst_avg);
	}

	float* dsqrtvar = new float[size];
	float sqrtvar = 1 / ivar;
	for (i = 0; i < size; i++) {
		dsqrtvar[i] = divar[i] * (-1 / sqrtvar * sqrtvar);
	}

	float* dvar = new float[size];
	float var = dst_dist * dst_dist;
	for (i = 0; i < size; i++) {
		dvar[i] = 0.5 * dsqrtvar[i] / sqrt(var + 10e-7);
	}

	float* dsq = new float[size];
	for (i = 0; i < size; i++) {
		dsq[i] = dvar[i];
	}

	float* dxu2 = new float[size];
	float xu = dst_dist;
	for (i = 0; i < size; i++) {
		dxu2[i] = 2 * xu * dsq[i];
	}

	float* dx1 = new float[size];
	float* du = new float[size];
	for (i = 0; i < size; i++) {
		dx1[i] = dxu1[i] + dxu2[i];
		du[i] = -dx1[i];
	}

	float* dx2 = new float[size];
	for (i = 0; i < size; i++) {
		dx2[i] = du[i];
	}

	float* dx = new float[size];
	for (i = 0; i < size; i++) {
		dx[i] = dx1[i] + dx2[i];
	}

	for (i = 0; i < size; i++) {
		delta_host[i] = dx[i];
	}	

	cpy_dev(delta, delta_host, size);

	b += db * lr;
	g += dg * lr;

	delete[] delta_host;
	delete[] data_host;
	delete[] dxu1;
	delete[] divar;
	delete[] dsqrtvar;
	delete[] dvar;
	delete[] dsq;
	delete[] dxu2;
	delete[] dx1;
	delete[] du;
	delete[] dx2;
	delete[] dx;
}
	
		
(int size, float avg, float* data, float* data_sub_avg, float* data_sub_avg_sq)



void set_input_label() {
	inputs = new vector<float*>();
	labels = new vector<int>();
	answers = new vector<float*>();

	float* x1 = new float[3];
	x1[0] = 0;
	x1[1] = 0;
	x1[2] = 0;
	inputs->push_back(x1);

	float* x2 = new float[3];
	x2[0] = 0;
	x2[1] = 0;
	x2[2] = 1;
	inputs->push_back(x2);	

	float* x3 = new float[3];
	x3[0] = 0;
	x3[1] = 1;
	x3[2] = 0;
	inputs->push_back(x3);	

	float* x4 = new float[3];
	x4[0] = 0;
	x4[1] = 1;
	x4[2] = 1;
	inputs->push_back(x4);

	float* x5 = new float[3];
	x5[0] = 1;
	x5[1] = 0;
	x5[2] = 0;
	inputs->push_back(x5);

	float* x6 = new float[3];
	x6[0] = 1;
	x6[1] = 0;
	x6[2] = 1;
	inputs->push_back(x6);

	float* x7 = new float[3];
	x7[0] = 1;
	x7[1] = 1;
	x7[2] = 0;
	inputs->push_back(x7);

	float* x8 = new float[3];
	x8[0] = 1;
	x8[1] = 1;
	x8[2] = 1;
	inputs->push_back(x8);

	int i, j;
	
	for (int i = 0; i < 8; i++) {
		labels->push_back(i);
		float* arr = new float[8];
		memset(arr, 0, 8 * sizeof(float));
		arr[i] = 1;
		answers->push_back(arr);
		for (int j = 0; j < 8; j++) {
			printf("%f ", arr[j]);
		}		
	}	
}		
		
		
		