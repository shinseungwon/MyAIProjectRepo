#include "cuda_runtime.h"
#include "device_launch_parameters.h"

#include <iostream>
#include <stdio.h>
#include <fstream>
#include <vector>
#include <string>
#include <chrono>
#include <random>

using namespace std;

//prepare test data
#define IMAGE_PATH "mnist\\train-images.idx3-ubyte"
#define LABEL_PATH "mnist\\train-labels.idx1-ubyte"

//#define BIAS_WEIGHT_PATH "mnist\\biasweight.txt" //learned data
#define BIAS_WEIGHT_PATH "mnist\\bwvalue.txt" //normalized data

#define IMAGE_HEADER_SIZE 16
#define LABEL_HEADER_SIZE 8

#define TOTAL_SIZE 1000 // <= 60000
#define MINI_BATCH_SIZE 200 // <= TOTAL_IMG_SIZE
//~prepare test data

#define LEVEL_SIZE 3

#define LEARN_COUNT 10
#define LEARN_RATE 0.1 // sigmoid : 0.1, relu : 0.000001

vector<float*>* images;
vector<int>* labels;
vector<int>* mbatch;

float* level[LEVEL_SIZE + 1];
int level_size[LEVEL_SIZE + 1]{ 784, 50, 100, 10 };

float** bss;
float** wss;

float* output;
float* output_dev;

float* bss_dev[LEVEL_SIZE];
float* wss_dev[LEVEL_SIZE];

float** wsst = new float* [LEVEL_SIZE];
float** wssd = new float* [LEVEL_SIZE];
float** xssd = new float* [LEVEL_SIZE + 1];

int cnt = 0;
int correct = 0;
int anstmp = 0;
float cee;
float* ans;

void read_image_label();
void read_bias_weight();
void make_bias_weight();

void mini_batch(int n, int count);
void predict(float* image, int label);
void backprop(float* img, int label);
float cross_entropy_error(float* y, float* t, int n);

__global__ void softmax(float* a, float* b, int n);
__global__ void matrix_multiplication(float* a, int m, int n, float* b, int o, int p, float* res);
__global__ void bias_sigmoid(float* a, float* bias);
__global__ void bias_relu(float* a, float* bias);
__global__ void sigmoid_backward(float* dy, float* y, int m, int n);
__global__ void relu_backward(float* dy, float* y, int m, int n);
__global__ void set_weight_changes(float* ws, float* wds, int h, int w);
__global__ void set_bias_changes(float* bs, float* bds, int n);
__global__ void matrix_transpose(float* a, int m, int n, float* b);
__global__ void print_matrix(float* a, int n);

int main()
{
	//initialize
	srand(static_cast<unsigned int>(time(NULL)));
	int i, j, k, size;

	images = new vector<float*>();
	labels = new vector<int>();
	read_image_label();

	bss = new float* [LEVEL_SIZE];
	wss = new float* [LEVEL_SIZE];

	read_bias_weight();
	//make_bias_weight();

	mini_batch(TOTAL_SIZE, MINI_BATCH_SIZE);

	cudaMalloc((void**)&level[0], level_size[0] * sizeof(float));
	cudaMalloc((void**)&xssd[0], level_size[0] * sizeof(float));
	cudaMalloc((void**)&output_dev, level_size[LEVEL_SIZE] * sizeof(float));
	output = new float[level_size[3]];
	ans = new float[level_size[3]];

	for (i = 0; i < LEVEL_SIZE; i++) {
		size = level_size[i + 1] * sizeof(float);
		cudaMalloc((void**)&bss_dev[i], size);
		cudaMemcpy(bss_dev[i], bss[i], size, cudaMemcpyHostToDevice);
		cudaMalloc((void**)&level[i + 1], size);
		cudaMalloc((void**)&xssd[i + 1], size);

		size = level_size[i] * level_size[i + 1] * sizeof(float);
		cudaMalloc((void**)&wss_dev[i], size);
		cudaMemcpy(wss_dev[i], wss[i], size, cudaMemcpyHostToDevice);
		cudaMalloc((void**)&wsst[i], size);
		cudaMalloc((void**)&wssd[i], size);
	}
	//~initialize

	//work
	for (i = 0; i < MINI_BATCH_SIZE; i++) {
		cout << "backprop " << i;

		//cout << endl;
		//for (j = 0; j < 28; j++) {
		//	for (k = 0; k < 28; k++) {
		//		cout << (images->at(mbatch->at(i))[j * 28 + k] > 0) << ' ';
		//	}
		//	cout << endl;
		//}

		predict(images->at(mbatch->at(i)), labels->at(mbatch->at(i)));
		cout << " cee : " << cee;
		backprop(images->at(mbatch->at(i)), labels->at(mbatch->at(i)));
		cout << " to " << cee << endl;
	}

	cnt = 0;
	correct = 0;
	for (i = 0; i < TOTAL_SIZE; i++) {
		predict(images->at(i), labels->at(i));
		cnt++;
		if (anstmp == labels->at(i)) {
			correct++;
		}
		cout << i << " predict : " << anstmp << " answer : " << labels->at(i) << " cee : " << cee << " data : ";
		for (j = 0; j < 10; j++) {
			cout << output[j] << ' ';
		}
		cout << endl;
	}
	cout << "accuracy " << correct / (float)cnt << endl;
	//~work

	//free memory
	for (i = 0; i < LEVEL_SIZE; i++) {
		cudaFree(bss_dev[i]);
		cudaFree(wss_dev[i]);
		cudaFree(wsst[i]);
		cudaFree(wssd[i]);
		cudaFree(level[i]);
		cudaFree(xssd[i]);
	}
	cudaFree(level[3]);
	cudaFree(xssd[3]);

	cudaFree(bss_dev);
	cudaFree(wss_dev);
	cudaFree(wsst);
	cudaFree(wssd);
	cudaFree(level);
	cudaFree(xssd);
	//~free memory

	return 0;
}

void read_image_label() {
	int header = 0, row = 0, col = 0, n, m, i = 0, j = 0, k = 0, l = 0, count = 0;

	//read image
	ifstream input_image(IMAGE_PATH, ios::binary);
	vector<char> bytes_i;
	char headerbuffer[IMAGE_HEADER_SIZE];
	input_image.read(headerbuffer, IMAGE_HEADER_SIZE);
	for (j = 0; j < IMAGE_HEADER_SIZE; j++) {
		bytes_i.push_back(headerbuffer[j]);
	}

	for (j = 0; j < TOTAL_SIZE; j++) {
		char* imagebuffer = new char[level_size[0]];
		input_image.read(imagebuffer, level_size[0]);
		for (k = 0; k < level_size[0]; k++) {
			bytes_i.push_back(imagebuffer[k]);
		}
	}

	//read label
	ifstream input_label(LABEL_PATH, ios::binary);
	vector<char> bytes_l;
	char labelbuffer[LABEL_HEADER_SIZE + TOTAL_SIZE];
	input_label.read(labelbuffer, LABEL_HEADER_SIZE + TOTAL_SIZE);
	for (j = 0; j < LABEL_HEADER_SIZE + TOTAL_SIZE; j++) {
		bytes_l.push_back(labelbuffer[j]);
	}

	n = bytes_i.size();
	m = bytes_l.size();

	cout << "image header : ";
	for (j = 0; j < IMAGE_HEADER_SIZE; j++) {
		cout << (int)(unsigned char)bytes_i[i++] << ' ';
	}
	cout << endl;

	cout << "label header : ";
	for (j = 0; j < LABEL_HEADER_SIZE; j++) {
		cout << (int)(unsigned char)bytes_l[l++] << ' ';
	}
	cout << endl;

	float* img;
	while (i < n && l < m && count < TOTAL_SIZE) {
		labels->push_back((int)(unsigned char)bytes_l[l++]);
		img = new float[level_size[0]];

		for (j = 0; j < level_size[0]; j++) {
			img[j] = (float)(unsigned char)bytes_i[i++];
		}
		images->push_back(img);
		count++;
	}

	cout << images->size() << " images" << endl;

	input_image.close();
	input_label.close();
}

void read_bias_weight() {
	ifstream bias_wieght(BIAS_WEIGHT_PATH);
	vector<char> chars(istreambuf_iterator<char>(bias_wieght), (istreambuf_iterator<char>()));
	int n = chars.size(), i, j, k, q = 0;
	vector<vector<float>*>* vs = new vector<vector<float>*>();
	vector<float>* v = new vector<float>();
	string s = "";
	for (i = 0; i < n; i++) {
		if (chars[i] == '\n') {
			vs->push_back(v);
			v = new vector<float>();
		}
		else if (chars[i] == '/') {
			v->push_back(stof(s));
			s.clear();
		}
		else {
			s += chars[i];
		}
	}

	for (i = 0; i < LEVEL_SIZE; i++) {
		bss[i] = &vs->at(i)->at(0);
		q++;
	}
	for (i = 0; i < LEVEL_SIZE; i++) {
		int size = level_size[i] * level_size[i + 1];
		float* x = new float[size];

		int pos = 0;
		for (j = 0; j < level_size[i]; j++) {
			memcpy(&x[pos], &vs->at(q++)->at(0), level_size[i + 1] * sizeof(float));
			pos += level_size[i + 1];
		}
		wss[i] = x;
	}
}

void make_bias_weight() {
	int i, j, size;
	float sd;
	float dist[10];

	for (i = 0; i < LEVEL_SIZE; i++) {
		bss[i] = new float[level_size[i + 1]];
		memset(bss[i], 0, level_size[i + 1] * sizeof(float));
		size = level_size[i] * level_size[i + 1];
		wss[i] = new float[size];
		sd = sqrt(1 / (float)level_size[i + 1]);
		//sd = sqrt(2 / (float)level_size[i + 1]);		
		normal_distribution<float> distribution(0.5, sd);
		default_random_engine generator;
		generator.seed(rand());
		memset(dist, 0, 10 * sizeof(float));
		for (j = 0; j < size; j++) {
			wss[i][j] = distribution(generator);
			if (wss[i][j] > 1) {
				wss[i][j] = 1 - 10e-7;
			}
			else if (wss[i][j] < 0) {
				wss[i][j] = 10e-7;
			}

			dist[(int)(wss[i][j] * 10)]++;
		}

		cout << "sd : " << sd << ", size : " << size << endl;
		for (j = 0; j < 10; j++) {
			cout << j * 0.1 << "~ : " << dist[j] << '(' << dist[j] * 100 / (float)size << "%)" << endl;
		}
		cout << endl;
	}
}

void mini_batch(int n, int count) {
	int i, x;
	mbatch = new vector<int>();
	int* arr = new int[n];
	for (i = 0; i < n; i++) {
		arr[i] = i;
	}
	for (i = n; i > n - count; i--) {
		x = rand() % i;
		mbatch->push_back(arr[x]);
		arr[x] = arr[i - 1];
	}
	delete[] arr;
}

void predict(float* image, int label) {
	int i;

	cudaMemcpy(level[0], image, level_size[0] * sizeof(float), cudaMemcpyHostToDevice);

	for (i = 0; i < LEVEL_SIZE - 1; i++) {
		matrix_multiplication << <1, level_size[i + 1] >> > (level[i], 1, level_size[i], wss_dev[i], level_size[i], level_size[i + 1], level[i + 1]);
		bias_sigmoid << <1, level_size[i + 1] >> > (level[i + 1], bss_dev[i]);
		//bias_relu << <1, level_size[i + 1] >> > (level[i + 1], bss_dev[i]);			
	}

	matrix_multiplication << <1, level_size[LEVEL_SIZE] >> > (level[LEVEL_SIZE - 1], 1, level_size[LEVEL_SIZE - 1], wss_dev[LEVEL_SIZE - 1], level_size[LEVEL_SIZE - 1], level_size[LEVEL_SIZE], level[LEVEL_SIZE]);
	softmax << <1, 1 >> > (level[LEVEL_SIZE], output_dev, level_size[LEVEL_SIZE]);
	cudaMemcpy(output, output_dev, level_size[LEVEL_SIZE] * sizeof(float), cudaMemcpyDeviceToHost);

	memset(ans, 0, level_size[LEVEL_SIZE] * sizeof(float));
	ans[label] = 1;

	cee = cross_entropy_error(output, ans, level_size[LEVEL_SIZE]);

	int maxidx = -1;
	float max = -1;
	for (i = 0; i < level_size[LEVEL_SIZE]; i++) {
		if (output[i] > max) {
			max = output[i];
			maxidx = i;
		}
	}

	anstmp = maxidx;
}

void backprop(float* img, int label) {
	int i, j;

	for (i = 0; i < LEARN_COUNT; i++) {
		predict(img, label);
		//cout << cee << endl;
		for (j = 0; j < LEVEL_SIZE; j++) {
			matrix_transpose << <level_size[j], level_size[j + 1] >> > (wss_dev[j], level_size[j], level_size[j + 1], wsst[j]);
		}

		for (j = 0; j < level_size[LEVEL_SIZE]; j++) {
			output[j] -= ans[j];
		}
		cudaMemcpy(xssd[LEVEL_SIZE], output, level_size[LEVEL_SIZE] * sizeof(float), cudaMemcpyHostToDevice);
		for (j = 0; j < level_size[LEVEL_SIZE]; j++) {
			output[j] += ans[j];
		}

		// 3 = j, 2 = j - 1
		for (j = LEVEL_SIZE; j > 1; j--) {
			matrix_multiplication << <1, level_size[j - 1] >> > (xssd[j], 1, level_size[j], wsst[j - 1], level_size[j], level_size[j - 1], xssd[j - 1]);
			matrix_multiplication << <level_size[j - 1], level_size[j] >> > (level[j - 1], level_size[j - 1], 1, xssd[j], 1, level_size[j], wssd[j - 1]);
			sigmoid_backward << <1, level_size[j - 1] >> > (xssd[j - 1], level[j - 1], 1, level_size[j - 1]);
			//relu_backward << <1, level_size[j - 1] >> > (xssd[j - 1], level[j - 1], 1, level_size[j - 1]);
		}

		matrix_multiplication << <level_size[0], level_size[1] >> > (level[0], level_size[0], 1, xssd[1], 1, level_size[1], wssd[0]);

		for (j = 0; j < LEVEL_SIZE; j++) {
			set_weight_changes << < level_size[j], level_size[j + 1] >> > (wss_dev[j], wssd[j], level_size[j], level_size[j + 1]);
			set_bias_changes << <1, level_size[j + 1] >> > (bss_dev[j], xssd[j + 1], level_size[j + 1]);
		}
	}
}

float cross_entropy_error(float* y, float* t, int n) {
	float res = 0;
	int i;

	for (i = 0; i < n; i++) {
		res += t[i] * log(y[i] + 1e-7);
	}

	return -res;
}

__global__ void softmax(float* a, float* b, int n) {
	int i;
	float max = 0, expsum = 0;
	for (i = 0; i < n; i++) {
		if (a[i] > max) {
			max = a[i];
		}
		b[i] = a[i];
	}
	for (i = 0; i < n; i++) {
		b[i] -= max;
		expsum += exp(b[i]);
	}
	for (i = 0; i < n; i++) {
		b[i] = exp(b[i]) / expsum;
	}
}
//row-col <<<m, p>>> matrix, {n == o} (m x n) x (o x p) = (m x p)
__global__ void matrix_multiplication(float* a, int m, int n, float* b, int o, int p, float* res) {
	int bi = blockIdx.x, ti = threadIdx.x, sb = bi * n, st = ti, c = bi * p + ti, i;
	res[c] = 0;
	for (i = 0; i < n; i++) {
		res[c] += a[sb + i] * b[st];
		st += p;
	}
}

__global__ void bias_sigmoid(float* a, float* bias) {
	int ti = threadIdx.x;
	a[ti] = 1 / (1 + exp(-(a[ti] + bias[ti])));
}

__global__ void bias_relu(float* a, float* bias) {
	int ti = threadIdx.x;
	a[ti] = a[ti] <= 0 ? 0 : (a[ti] + bias[ti]);
}

__global__ void sigmoid_backward(float* dy, float* y, int m, int n) {
	int i = blockIdx.x, j = threadIdx.x, seq = n * i + j;
	float t = y[seq];
	dy[seq] *= t * (1 - t);
}

__global__ void relu_backward(float* dy, float* y, int m, int n) {
	int i = blockIdx.x, j = threadIdx.x, seq = n * i + j;
	dy[seq] *= (y[seq] <= 0) ? 0 : 1;
}

__global__ void set_weight_changes(float* ws, float* wds, int h, int w) {
	int bx = blockIdx.x;
	int tx = threadIdx.x;
	int seq = bx * w + tx;
	ws[seq] -= wds[seq] * LEARN_RATE;
}

__global__ void set_bias_changes(float* bs, float* bds, int n) {
	int bx = blockIdx.x;
	int tx = threadIdx.x;
	bs[tx] -= bds[tx] * LEARN_RATE;
}

__global__ void matrix_transpose(float* a, int m, int n, float* b) {
	int j = blockIdx.x;
	int i = threadIdx.x;
	b[i * m + j] = a[j * n + i];
}

__global__ void print_matrix(float* a, int n) {
	printf("start print(%d items)\n", n);
	for (int i = 0; i < n; i++) {
		printf("%f ", a[i]);
		if (isnan(a[i])) {
			printf("nanan");
		}
	}
	printf("\n");
}